# Medical Chatbot

A medical domain-specific chatbot implementation with two distinct architectures: LangChain-based for evaluation and prompt engineering, and CrewAI-based for agentic AI demonstration. The system includes a comprehensive evaluation framework for assessing different LLM models and prompt configurations.

<img src="docs/figs/medical_chatbot.png" alt="Medical Chatbot Architecture" height="400px">
(This image is generated by ChatGPT)

## Architecture Overview

### 1. LangChain Implementation
- **Core Components**:
  - Document Processing Pipeline
  - Vector Store Integration (FAISS)
  - Retrieval-Augmented Generation (RAG)
  - Custom Chain Implementations
  - Evaluation Framework

- **Key Features**:
  - Document chunking and embedding
  - Semantic search capabilities
  - Context-aware response generation
  - Prompt template management
  - Automated evaluation pipeline

### 2. CrewAI Implementation
- **Core Components**:
  - Multi-agent System
  - Task Orchestration
  - Role-based Specialization
  - Inter-agent Communication

- **Key Features**:
  - Agent-based conversation flow
  - Task delegation and coordination
  - Specialized medical knowledge agents
  - Dynamic conversation management

## Installation

1. Clone and setup:
```bash
git clone https://github.com/sungcheolkim78/chatbot-humana.git
cd chatbot-humana
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Environment Configuration:
Copy env_example to `.env` file and update the api keys.
`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY` is used only for the evaluation
dataset generation and chatbot response evaluation.

4. Install and download open-source LLMs:
Here, we use ollama for the local LLM provider with open-source LLMs. 
For the setup, please check [this document](docs/ollama_installation.md)

## Usage

The project uses Makefile for common operations:

### LangChain Version
```bash
# Start the chatbot
make chatbot_langchain

# Generate evaluation dataset
make eval_dataset

# Launch evaluation web interface
make eval_app

# Run batch evaluation
make eval_batch
```

### CrewAI Version
```bash
# Start the agentic chatbot
make chatbot_crewai
```

## Evaluation Framework

### 1. Response Quality Metrics
- [x] Relevance: Semantic alignment with query intent
- [x] Coherence: Logical flow and consistency
- [x] Accuracy: Factual correctness and precision

### 2. Performance Metrics
- [x] Response Time: Latency measurements
- [ ] Resource Utilization: Memory and CPU profiling
- [ ] Error Rates: Failure analysis

### 3. User Experience Metrics
- [ ] User Feedback: Structured feedback collection
- [ ] Friendliness and Engagement: Interaction quality
- [ ] Knowledge Adaptation: User expertise level handling

Detailed evaluation dataset generation and score calculation can be found [here](evaluation/README.md)

## Model Performance Analysis

We have conducted extensive evaluation of various open-source LLM models across multiple dimensions:

![](docs/figs/metrics_boxplot_by_model.png)

Detailed performance analysis and methodology can be found in [Open Source Model Performance](docs/opensource_model_performance.md).

### Key Findings:
1. Model Performance Comparison
   - Response Quality
   - Latency Analysis
   - Error Patterns

2. Prompt Engineering Impact
   - Template Effectiveness
   - Context Utilization
   - Response Consistency

3. System Architecture Considerations
   - Scalability
   - Integration Complexity

## Development Guidelines

### Code Structure
```
medical_chatbot/
├── src/
│   ├── chatbot_langchain/
│   │   ├── app.py
│   │   ├── batch.py
│   │   └── components/
│   └── chatbot_crewai/
│       ├── main.py
│       ├── crew.py
│       └── config/
├── knowledge/
│   ├── slamon1987.pdf
│   └── slamon1987_claude.md
├── evaluation/
│   ├── configs/
│   ├── chatbot_results/
│   ├── datasets/
│   ├── components/
│   ├── dataset_generator.py
│   ├── app_eval.py
│   └── llm_scorer.py
├── docs/
│   ├── figs/
│   └── opensource_model_performance.md
├── tests/
└── Makefile
```

### Best Practices
1. Code Organization
   - Modular architecture
   - Clear separation of concerns
   - Comprehensive documentation

2. Testing
   - Unit tests for core components
   - Integration tests for workflows
   - Performance benchmarks

3. Documentation
   - API documentation
   - Architecture diagrams
   - Usage examples

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Submit a pull request

## Contact

For technical inquiries: sungcheol.kim78@gmail.com
