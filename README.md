# Medical Chatbot

[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![Status](https://img.shields.io/badge/status-active-success)]()

A medical domain-specific chatbot implementation with two distinct architectures: LangChain-based for evaluation and prompt engineering, and CrewAI-based for agentic AI demonstration. The system includes a comprehensive evaluation framework for assessing different LLM models and prompt configurations.

## Table of Contents
- [Project Requirements](#project-requirements)
- [Architecture Overview](#architecture-overview)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Evaluation Framework](#evaluation-framework)
- [Model Performance Analysis](#model-performance-analysis)
- [Future Works](#future-works)
- [Development Guidelines](#development-guidelines)
- [Contributing](#contributing)
- [Contact](#contact)

<div align="center">
<img src="docs/figs/medical_chatbot.png" alt="Medical Chatbot Architecture" height="400px">
<p><em>(This image is generated by ChatGPT)</em></p>
</div>

## Project Requirements

The details of the project requirements can be found [here](docs/project_requirements.md)

## Architecture Overview

### 1. LangChain Implementation

- **Core Components**:
  - Document Processing Pipeline
  - Vector Store Integration (FAISS)
  - Retrieval-Augmented Generation (RAG)
  - Custom Chain Implementations (LangGraph)
  - Evaluation Framework

- **Key Features**:
  - Document chunking and embedding
  - Semantic search capabilities
  - Context-aware response generation
  - Prompt template management
  - Automated evaluation pipeline

The details of the architectural design can be found [here](docs/architectural_design.md)

### 2. CrewAI Implementation

This implementation is premature. However, to demonstrate the difference between the agentic AI approach and conditional flow LLM pipeline, we include the chatbot version implemented by CrewAI. It provides basic chatbot functionality based on the knowledge base, but there is no evaluation pipeline.

- **Core Components**:
  - Multi-agent System
  - Task Orchestration
  - Role-based Specialization
  - Inter-agent Communication

- **Key Features**:
  - Agent-based conversation flow
  - Task delegation and coordination
  - Specialized medical knowledge agents
  
## Installation

### Prerequisites
- Python 3.9 or higher
- Git
- Virtual environment (recommended)
- Ollama (for local LLM support)

### Setup Steps

1. Clone and setup:
```bash
git clone https://github.com/sungcheolkim78/chatbot-medical.git
cd chatbot-medical
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Environment Configuration:
Create a `.env` file by copying `env_example`:
```bash
cp env_example .env
```
Update the following API keys in `.env`:
- `OPENAI_API_KEY`: For evaluation dataset generation
- `ANTHROPIC_API_KEY`: For chatbot response evaluation
- `GOOGLE_API_KEY`: For evaluation purposes

4. Install and download open-source LLMs:
For the ollama and LLM setup, please check [this document](docs/ollama_installation.md)

## Quick Start

To get started quickly with the LangChain version:

```bash
# Activate virtual environment
source .venv/bin/activate

# Start the chatbot
make chatbot_langchain
```

For more detailed usage instructions, see the [Usage](#usage) section below.

## Usage

The project uses Makefile for common operations:

### LangChain Version
```bash
source .venv/bin/activate

# Start the chatbot
make chatbot_langchain

# Generate evaluation dataset
make eval_dataset

# Launch evaluation dataset viewer
make eval_dataset_app

# Run batch evaluation
make eval_batch

# Launch LLM score viewer
make eval_score_app
```

### CrewAI Version
```bash
# Start the agentic chatbot
make chatbot_crewai
```

## Screenshot of the web applications

Chatbot (Langchain version) 

: The main chatbot application
![](docs/figs/chatbot_langchain_screenshot.png)

Dataset Viewer 

: The dataset viewer to validate the evaluation dataset with the source excerpts
![](docs/figs/eval_app_screenshot.png)

Chatbot Score Viewer 

: The chatbot response viewer with LLM Judge score
![](docs/figs/eval_score_screenshot.png)

## Evaluation Framework

We developed a tool to evaluate the chatbot performance so that continuous improvement can be possible. The initial step is to clean up the information for the knowledge base. We are using the seminal paper "Human Breast Cancer: Correlation of Relapse and Survival with Amplification of the HER-2/neu Oncogene". You can find the details of how to preprocess the PDF [here](docs/preprocess.md).

And here are the three main metrics for the evaluation. Check marks indicate implemented items and empty items are for future work. Currently all these metrics are scored by the SOTA LLM (Gemini-2.5-flash) independently. Human in the loop can be implemented through the feedback. Detailed evaluation dataset generation and score calculation can be found [here](docs/evaluation.md)

By continuously updating the prompts and measuring the metric improvement, we can improve the chatbot system incrementally. You can find the details of the continuous development [here](docs/continuous_development.md)

### 1. Factuality Metrics (Correctness)
- [x] Accuracy: Factual correctness and precision
- [x] Relevance: Semantic alignment with query intent
- [x] Coherence: Logical flow and consistency with given knowledge base

### 2. Performance Metrics (Response Time)
- [x] Response Time: Latency measurements
- [ ] Resource Utilization: Memory and CPU profiling
- [ ] Error Rates: Failure analysis

### 3. User Experience Metrics (Style)
- [x] Friendliness and Engagement: Interaction quality
- [x] Knowledge Adaptation: User expertise level handling
- [ ] User Feedback: Structured feedback collection

## Model Performance Analysis

We have conducted extensive evaluation of various open-source LLM models across multiple dimensions:

![](docs/figs/metrics_boxplot_by_model_v1.png)

Detailed performance analysis and methodology can be found in [Model Performance](docs/model_performance.md).

### Key Findings:

1. Factuality Check
   - Response Quality and Correctness based on the base LLM models and prompt engineering
   - Error Patterns

2. Prompt Engineering Impact
   - Template Effectiveness
   - Context Utilization
   - Response Consistency

3. System Architecture Considerations
   - Latency Analysis
   - Scalability
   - Integration Complexity

## Future Works

### 1. Integration of Unified Clinical Vocabulary Embeddings (ClinVec)

The current chatbot system's single-document knowledge base presents a significant limitation in handling multiple medical publications effectively. To address this, we propose a hierarchical retrieval architecture that leverages Unified Clinical Vocabulary Embeddings (ClinVec) through two distinct approaches: document-level and chunk-level retrieval methods. You can find the details [here](docs/clinvec.md).

### 2. Prompt engineering through continuous evaluation pipeline

### 3. Improving RAG system

- Try different LLM models
- Try different vector database (Milvus, Weaviate, Qdrant, Chroma)
- Parameter tuning on chunk size, chunk overlap, and text split strategy

## Development Guidelines

### Code Structure
```
medical_chatbot/
├── src/                      # Source code directory
│   ├── chatbot_langchain/    # LangChain implementation
│   │   ├── app.py            # Main application entry point
│   │   ├── batch.py          # Batch processing utilities
│   │   └── components/       # Core components and utilities
│   └── chatbot_crewai/       # CrewAI implementation
│       ├── main.py           # Main application entry point
│       ├── crew.py           # Crew configuration
│       └── config/           # Configuration files
├── knowledge/                # Knowledge base directory
│   ├── slamon1987.pdf        # Original research paper
│   └── slamon1987_claude.md  # Processed knowledge base
├── evaluation/               # Evaluation framework
│   ├── configs/              # Evaluation configurations
│   ├── chatbot_results/      # Evaluation results
│   ├── datasets/             # Evaluation datasets
│   ├── components/           # Evaluation components
│   ├── dataset_generator.py  # Dataset generation utilities
│   ├── app_eval.py           # Evaluation application
│   └── llm_scorer.py         # LLM scoring utilities
├── docs/                     # Documentation
│   ├── figs/                 # Figures and diagrams
│   └── README.md             # Documentation files
├── tests/                    # Test suite
├── README.md                 # Project documentation
└── Makefile                  # Build and utility commands
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Submit a pull request

## Contact

For technical inquiries: sungcheol.kim78@gmail.com
