# Evaluation of chatbot performance

## Dataset generation

1. Process pdf file. check [this document](../knowledge/README.md)
1. Create chunks for the source of the question/answer set
1. Based on the chunks, ask LLM to generate the proper question and answer pair
1. We can set the difficulty levels: easy, medium, and hard
1. As a final step, ask LLm to generate the confidence level for the answer
1. Save question and answer pair with meta data

You can create a new dataset by this command:
```bash
make eval_dataset # or python evaluation/dataset_generator.py
```

You can add two arguments; `paper_path` and `num_questions` so that you can define
the source knowledge file and how many questions for each categories. Here, we have
three LLM provider from OpenAI, Anthropic, Google and three difficulty levels. 

And when the `paper_path` has the pdf extension, it will use pdf loader and you can 
set different libraries. And if you provide markdown file generated by either 
OpenAI or Anthropic, it will use the unstructured markdown loader. 

Once it is generated, it will be saved `evaluation/datasets/dataset_{datetime}.json`

## Dataset Checker

To see the quality of the question and answer set, it is best to be evaluated by the 
domain experts. For that purpose, we prepared the streamlit web application. 

```bash
make eval_app
```

